---
title: Introduction
description: Welcome to the Ultimate AI-Powered Documentation Starter
---

# Vetradocs

**Vetradocs** is a next-generation documentation starter kit designed to provide a premium, AI-native experience out of the box. Built on top of **Fumadocs**, **LangChain**, and **Orama**, it combines a beautiful UI with powerful client-side RAG (Retrieval-Augmented Generation) search.

## Why "Vetradocs"?

Traditional documentation sites are static. Users search for keywords and hope for matches. **Vetradocs** changes the game by embedding a context-aware AI Assistant directly into the reading experience.

### Key Features

- **üß† Context-Aware AI Chat**: An intelligent assistant that reads your documentation and answers user questions instantly.
- **‚ö° Client-Side RAG**: Powered by [Orama](https://askorama.ai/), search indexing happens at build time and runs incredibly fast in the browser or edge.
- **üí¨ Premium UI Experience**:
    - **Floating Action Bar**: A sleek, non-intrusive input bar that expands as you type.
    - **Rich Markdown Rendering**: The chat supports code blocks, bold text, lists, and links.
    - **Code Copying**: One-click copy for all code snippets generated by the AI.
- **üõ†Ô∏è Easy Configuration**: accurate control over the System Prompt, LLM Model (OpenAI, Ollama, Anthropic), and UI styling.
- **üöÄ Next.js 15 & React Server Components**: Built on the bleeding edge for maximum performance.

## Architecture

1.  **Ingestion (`scripts/build-index.mjs`)**: Scans your `.mdx` files at build time and creates a vector search index.
2.  **Storage**: The index is saved as a JSON file in `public/search-index.json`.
3.  **Retrieval (`app/api/chat/route.ts`)**: When a user asks a question, the API loads the index, finds relevant docs, and feeds them to the LLM.
4.  **Generation**: The LLM (via LangChain) generates a streaming response based *only* on your documentation.

## Next Steps

Ready to build? Check out the [Installation Guide](./installation) to get started.
